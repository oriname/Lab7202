{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cqfHR5rJt4h",
    "outputId": "0ba0f439-c58e-47d8-bb78-d202afb39459",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: Cython==0.29.32 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from gensim) (0.29.32)\n",
      "Requirement already satisfied: pandas in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7.1)\n",
      "Requirement already satisfied: simpful in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.10.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "AxkfffYtJ7nG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "RhSsvi3lNnYu"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Dictionary with 'data' as key and each review as an element of list\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ZOHKr6vcNwX2"
   },
   "outputs": [],
   "source": [
    "# list of articles\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "uI4Qfp5ENw5-"
   },
   "outputs": [],
   "source": [
    "# Dataframe with a column `document` containing all the articles\n",
    "news_df = pd.DataFrame({'document':documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGjLc78oN0Gb",
    "outputId": "2fae1c1c-9614-4237-bfa7-742916581485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document\n",
      "0  Well i'm not sure about the story nad it did s...\n",
      "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n",
      "2  Although I realize that principle is not one o...\n",
      "3  Notwithstanding all the legitimate fuss about ...\n",
      "4  Well, I will have to change the scoring on my ...\n"
     ]
    }
   ],
   "source": [
    "print(news_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7w6RVreHQTQs",
    "outputId": "4c2c62f7-1ec2-4436-ac5d-dcd0d66accb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oriname\\AppData\\Local\\Temp\\ipykernel_23476\\4185485833.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tokenized_doc = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = news_df[:100]\n",
    "tokenized_doc = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jZUPbA3QV0s",
    "outputId": "c71bb808-6a1b-49f2-b5ed-5fcf03416016"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     well sure about story seem biased what disagre...\n",
       "1     yeah expect people read actually accept hard a...\n",
       "2     although realize that principle your strongest...\n",
       "3     notwithstanding legitimate fuss about this pro...\n",
       "4     well will have change scoring playoff pool unf...\n",
       "                            ...                        \n",
       "95    follow mark last posting current investigation...\n",
       "96    could folk song clementine memory serves part ...\n",
       "97    first chip doesn that runs megabits second whi...\n",
       "98    created image gaea therefore must pinnacle cre...\n",
       "99    sale pontiac grand prix white white rims gray ...\n",
       "Name: document, Length: 100, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_doc(text):\n",
    "    #for token in text:\n",
    "    text =  ' '.join([w.lower() for w in text.split() if len(w)>3])\n",
    "    return text;\n",
    "\n",
    "tokenized_doc = tokenized_doc.apply(clean_doc)\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W1NAWAREQd5T",
    "outputId": "3b9931f1-bcdc-42b0-f3b2-6aca63110258"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Oriname\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Oriname\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     well sure story seem biased disagree statement...\n",
       "1     yeah expect people read actually accept hard a...\n",
       "2     although realize principle strongest points wo...\n",
       "3     notwithstanding legitimate fuss proposal much ...\n",
       "4     well change scoring playoff pool unfortunately...\n",
       "                            ...                        \n",
       "95    follow mark last posting current investigation...\n",
       "96    could folk song clementine memory serves part ...\n",
       "97    first chip runs megabits second beyond need vo...\n",
       "98    created image gaea therefore must pinnacle cre...\n",
       "99    sale pontiac grand prix white white rims gray ...\n",
       "Name: document, Length: 100, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "custom = list(stopwords_set)+list(punctuation)\n",
    "\n",
    "def stopWordRemoval(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = ' '.join([word for word in text if word not in custom])\n",
    "    return text\n",
    "\n",
    "tokenized_doc = tokenized_doc.apply(stopWordRemoval)\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vaNzmfTfQhTx",
    "outputId": "16563136-7dc6-411b-f080-dff2e569ef58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Oriname\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Oriname\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     well sure story seem biased disagree statement...\n",
       "1     yeah expect people read actually accept hard a...\n",
       "2     although realize principle strongest point wou...\n",
       "3     notwithstanding legitimate fuss proposal much ...\n",
       "4     well change scoring playoff pool unfortunately...\n",
       "                            ...                        \n",
       "95    follow mark last posting current investigation...\n",
       "96    could folk song clementine memory serf part go...\n",
       "97    first chip run megabit second beyond need voic...\n",
       "98    created image gaea therefore must pinnacle cre...\n",
       "99    sale pontiac grand prix white white rim gray i...\n",
       "Name: document, Length: 100, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing wordnet lemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('omw-1.4')\n",
    "def lemData(text):\n",
    "    text = word_tokenize(text)\n",
    "    newText = []\n",
    "    for word in text:\n",
    "        newText.append(lemmatizer.lemmatize(word))\n",
    "    return ' '.join(newText)\n",
    "\n",
    "tokenized_doc = tokenized_doc.apply(lemData)\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "4ts_r2OvSMxL",
    "outputId": "66287709-7359-4158-dd84-4f4cf686c56d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126337</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.17339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122917</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2         3    4    5    6         7    8    9    ...  990  991  \\\n",
       "0   0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "1   0.0  0.0  0.0  0.196228  0.0  0.0  0.0  0.167434  0.0  0.0  ...  0.0  0.0   \n",
       "2   0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "3   0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "4   0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "..  ...  ...  ...       ...  ...  ...  ...       ...  ...  ...  ...  ...  ...   \n",
       "95  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "96  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "97  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "98  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "99  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "    992  993  994       995      996  997  998  999  \n",
       "0   0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.126337  0.00000  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "..  ...  ...  ...       ...      ...  ...  ...  ...  \n",
       "95  0.0  0.0  0.0  0.000000  0.17339  0.0  0.0  0.0  \n",
       "96  0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "97  0.0  0.0  0.0  0.122917  0.00000  0.0  0.0  0.0  \n",
       "98  0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "99  0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "\n",
       "[100 rows x 1000 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialising the tfidf vectorizer with the default stopword list \n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features= 1000, max_df = 0.5, smooth_idf=True)\n",
    "\n",
    "#Vectorizing 'X' column\n",
    "vector =tfidf.fit_transform(tokenized_doc)\n",
    "\n",
    "#Converting vector into an array\n",
    "X= vector.toarray()\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3D4_FDTZSr_Q",
    "outputId": "8e90a74b-be62-48d3-9c98-9b00f7b7e145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of topics chosen are 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=5, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)\n",
    "print(\"The number of topics chosen are\",len(svd_model.components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrRacUn6S5Cz",
    "outputId": "6c7a279b-9f2a-4029-a238-927d21250200"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNqG4BLETAC3",
    "outputId": "25ea0b8c-744d-4ff5-f6a5-242de9ace082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Topic 0: ', 'like', 'know', 'chip', 'think', 'time', 'good', 'year']\n",
      "['Topic 1: ', 'memory', 'cache', 'vram', 'simm', 'chip', 'simms', 'song']\n",
      "['Topic 2: ', 'chip', 'phone', 'government', 'clipper', 'voice', 'key', 'work']\n",
      "['Topic 3: ', 'israel', 'israeli', 'arab', 'memory', 'table', 'medium', 'know']\n",
      "['Topic 4: ', 'vram', 'need', 'jesus', 'really', 'government', 'appears', 'simms']\n"
     ]
    }
   ],
   "source": [
    "terms = tfidf.get_feature_names_out()\n",
    "topics = []\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    topics.append(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        topics.append(t[0])\n",
    "\n",
    "final_topic_list = [topics[i:i+8] for i in range(0, len(topics), 8)]\n",
    "\n",
    "for x in final_topic_list:\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2clqwMevWHDT"
   },
   "source": [
    "Gensim LSI & **LDA** **bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "bLxsZ-ToUwgO"
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BC3KQ3erW2Q4",
    "outputId": "725cbe76-68ab-4276-ad91-2c1df58e3efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['well', 'sure', 'story', 'seem', 'biased', 'disagree', 'statement', 'medium', 'ruin', 'israel', 'reputation', 'rediculous', 'medium', 'israeli', 'medium', 'world', 'lived', 'europe', 'realize', 'incidence', 'described', 'letter', 'occured', 'medium', 'whole', 'seem', 'ignore', 'subsidizing', 'israel', 'existance', 'european', 'least', 'degree', 'think', 'might', 'reason', 'report', 'clearly', 'atrocity', 'shame', 'austria', 'daily', 'report', 'inhuman', 'act', 'commited', 'israeli', 'soldier', 'blessing', 'received', 'government', 'make', 'holocaust', 'guilt', 'away', 'look', 'jew', 'treating', 'race', 'power', 'unfortunate']]\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords_set\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize and remove the stopwords\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "# Creating a list of documents \n",
    "list_of_docs = tokenized_doc.tolist()\n",
    "\n",
    "# Implementing the function for list_of_docs\n",
    "doc_clean = [clean(doc).split() for doc in list_of_docs]\n",
    "print(doc_clean[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_2NYr9nYBIf",
    "outputId": "960cfc4a-2cb8-440d-e136-de5b32649ddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.435*\"graphic\" + 0.249*\"mail\" + 0.228*\"image\" + 0.224*\"file\" + '\n",
      "  '0.191*\"send\" + 0.174*\"object\" + 0.158*\"format\" + 0.157*\"server\" + '\n",
      "  '0.137*\"also\" + 0.124*\"tracer\"'),\n",
      " (1,\n",
      "  '-0.463*\"israel\" + -0.322*\"israeli\" + -0.205*\"soldier\" + -0.204*\"lebanese\" + '\n",
      "  '-0.183*\"peace\" + -0.181*\"village\" + -0.136*\"zone\" + -0.133*\"would\" + '\n",
      "  '-0.122*\"people\" + -0.118*\"time\"'),\n",
      " (2,\n",
      "  '0.231*\"know\" + 0.218*\"would\" + 0.216*\"like\" + 0.210*\"good\" + '\n",
      "  '-0.198*\"israel\" + 0.194*\"patch\" + 0.170*\"contact\" + 0.170*\"bike\" + '\n",
      "  '0.145*\"life\" + 0.143*\"jesus\"'),\n",
      " (3,\n",
      "  '0.348*\"adirondack\" + 0.280*\"john\" + 0.278*\"providence\" + 0.278*\"cape\" + '\n",
      "  '0.278*\"rochester\" + 0.278*\"fredericton\" + 0.278*\"breton\" + '\n",
      "  '0.278*\"baltimore\" + 0.278*\"moncton\" + 0.278*\"utica\"'),\n",
      " (4,\n",
      "  '-0.293*\"patch\" + -0.267*\"contact\" + -0.248*\"bike\" + -0.216*\"force\" + '\n",
      "  '0.193*\"life\" + 0.188*\"good\" + 0.168*\"jesus\" + -0.163*\"motorcycle\" + '\n",
      "  '-0.160*\"wheel\" + -0.160*\"effect\"'),\n",
      " (5,\n",
      "  '-0.249*\"year\" + -0.239*\"moon\" + -0.185*\"lunar\" + -0.171*\"another\" + '\n",
      "  '-0.163*\"tonne\" + -0.145*\"could\" + -0.137*\"much\" + -0.131*\"payload\" + '\n",
      "  '-0.131*\"contest\" + 0.130*\"patch\"'),\n",
      " (6,\n",
      "  '0.279*\"phone\" + 0.194*\"criminal\" + 0.191*\"cipher\" + 0.157*\"mean\" + '\n",
      "  '0.154*\"government\" + 0.153*\"stolen\" + 0.139*\"public\" + 0.136*\"control\" + '\n",
      "  '0.133*\"key\" + 0.125*\"right\"'),\n",
      " (7,\n",
      "  '0.326*\"cable\" + 0.281*\"drive\" + 0.246*\"connector\" + 0.177*\"work\" + '\n",
      "  '0.170*\"power\" + 0.162*\"hard\" + 0.161*\"disk\" + 0.160*\"spare\" + 0.147*\"back\" '\n",
      "  '+ 0.144*\"another\"'),\n",
      " (8,\n",
      "  '-0.281*\"andrew\" + -0.280*\"conference\" + -0.279*\"paper\" + '\n",
      "  '-0.279*\"application\" + -0.189*\"june\" + -0.167*\"mormon\" + '\n",
      "  '-0.140*\"construction\" + -0.140*\"tutorial\" + -0.140*\"programmer\" + '\n",
      "  '-0.140*\"theme\"'),\n",
      " (9,\n",
      "  '-0.319*\"mormon\" + 0.200*\"chip\" + 0.199*\"session\" + 0.197*\"key\" + '\n",
      "  '-0.193*\"belief\" + 0.192*\"door\" + 0.179*\"back\" + -0.149*\"word\" + '\n",
      "  '0.136*\"secret\" + -0.133*\"believe\"')]\n"
     ]
    }
   ],
   "source": [
    "# Creating the dictionary id2word from our cleaned word list doc_clean\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Creating the corpus\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the LSi model\n",
    "lsimodel = LsiModel(corpus=doc_term_matrix, num_topics=10, id2word=dictionary)\n",
    "pprint(lsimodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "al0Y1xmKYKbc",
    "outputId": "b33e14f4-0b53-4518-aac8-808a0915f64a"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Creating the dictionary id2word from our cleaned word list doc_clean\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Creating the corpus\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the LDA model\n",
    "ldamodel = LdaModel(corpus=doc_term_matrix, num_topics=20,id2word=dictionary, random_state=20, passes=30)\n",
    "\n",
    "# printing the topics\n",
    "pprint(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cxt6XXMYOYX",
    "outputId": "4cda4148-238f-4b84-815c-b9af7b4b4e46"
   },
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "perplexity_lda = ldamodel.log_perplexity(doc_term_matrix)\n",
    "print('\\nPerplexity: ', perplexity_lda)  \n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "8CQ703aXZR5h",
    "outputId": "e0fb5fcd-0eac-4a45-c89f-e22481af7976"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pyplot \u001B[38;5;28;01mas\u001B[39;00m plt\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwordcloud\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WordCloud, STOPWORDS\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolors\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmcolors\u001B[39;00m\n\u001B[0;32m      5\u001B[0m cols \u001B[38;5;241m=\u001B[39m [color \u001B[38;5;28;01mfor\u001B[39;00m name, color \u001B[38;5;129;01min\u001B[39;00m mcolors\u001B[38;5;241m.\u001B[39mTABLEAU_COLORS\u001B[38;5;241m.\u001B[39mitems()]  \u001B[38;5;66;03m# more colors: 'mcolors.XKCD_COLORS'\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stopwords,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = ldamodel.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "fig.savefig('word_cloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.8.2.2.tar.gz (220 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (1.24.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (3.6.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (22.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Building wheels for collected packages: wordcloud\n",
      "  Building wheel for wordcloud (setup.py): started\n",
      "  Building wheel for wordcloud (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for wordcloud\n",
      "Failed to build wordcloud\n",
      "Installing collected packages: wordcloud\n",
      "  Running setup.py install for wordcloud: started\n",
      "  Running setup.py install for wordcloud: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [26 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\tokenization.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\_version.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\__init__.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\__main__.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\stopwords -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  UPDATING build\\lib.win-amd64-cpython-311\\wordcloud/_version.py\n",
      "  set build\\lib.win-amd64-cpython-311\\wordcloud/_version.py to '1.8.2.2'\n",
      "  running build_ext\n",
      "  building 'wordcloud.query_integral_image' extension\n",
      "  creating build\\temp.win-amd64-cpython-311\n",
      "  creating build\\temp.win-amd64-cpython-311\\Release\n",
      "  creating build\\temp.win-amd64-cpython-311\\Release\\wordcloud\n",
      "  \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.34.31933\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\include -IC:\\Users\\Oriname\\AppData\\Local\\Programs\\Python\\Python311\\include -IC:\\Users\\Oriname\\AppData\\Local\\Programs\\Python\\Python311\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.34.31933\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" /Tcwordcloud/query_integral_image.c /Fobuild\\temp.win-amd64-cpython-311\\Release\\wordcloud/query_integral_image.obj\n",
      "  query_integral_image.c\n",
      "  C:\\Users\\Oriname\\AppData\\Local\\Programs\\Python\\Python311\\include\\pyconfig.h(59): fatal error C1083: Cannot open include file: 'io.h': No such file or directory\n",
      "  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.34.31933\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for wordcloud\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for wordcloud did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [28 lines of output]\n",
      "  running install\n",
      "  C:\\Users\\Dev\\Desktop\\Lab7202\\venv\\Lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\tokenization.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\_version.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\__init__.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\__main__.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\stopwords -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "  UPDATING build\\lib.win-amd64-cpython-311\\wordcloud/_version.py\n",
      "  set build\\lib.win-amd64-cpython-311\\wordcloud/_version.py to '1.8.2.2'\n",
      "  running build_ext\n",
      "  building 'wordcloud.query_integral_image' extension\n",
      "  creating build\\temp.win-amd64-cpython-311\n",
      "  creating build\\temp.win-amd64-cpython-311\\Release\n",
      "  creating build\\temp.win-amd64-cpython-311\\Release\\wordcloud\n",
      "  \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.34.31933\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\include -IC:\\Users\\Oriname\\AppData\\Local\\Programs\\Python\\Python311\\include -IC:\\Users\\Oriname\\AppData\\Local\\Programs\\Python\\Python311\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.34.31933\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" /Tcwordcloud/query_integral_image.c /Fobuild\\temp.win-amd64-cpython-311\\Release\\wordcloud/query_integral_image.obj\n",
      "  query_integral_image.c\n",
      "  C:\\Users\\Oriname\\AppData\\Local\\Programs\\Python\\Python311\\include\\pyconfig.h(59): fatal error C1083: Cannot open include file: 'io.h': No such file or directory\n",
      "  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.34.31933\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "wordcloud\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
