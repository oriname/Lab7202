{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  1. Write a python script to scrape tweets from twitter using snscrape library. The script should take the following inputs from the user:\n",
    "\n",
    "text = input('Enter query text to be matched (or leave it blank by pressing enter)')\n",
    "\n",
    "username = input('Enter specific username(s) from a twitter account without @ (or leave it blank by pressing enter): ')\n",
    "\n",
    "since = input('Enter startdate in this format yyyy-mm-dd (or leave it blank by pressing enter): ')\n",
    "\n",
    "until = input('Enter enddate in this format yyyy-mm-dd (or leave it blank by pressing enter): ')\n",
    "\n",
    "count = int(input('Enter max number of tweets or enter -1 to retrieve all possible tweets: '))\n",
    "\n",
    "retweet = input('Exclude Retweets? (y/n): ')\n",
    "\n",
    "replies = input('Exclude Replies? (y/n): ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  2. The script should save the scraped tweets in a csv:\n",
    "\n",
    "def search(text, username, since, until, retweet, replies):\n",
    "    global filename\n",
    "    q = text\n",
    "    if username != '':\n",
    "        q += f\" from:{username}\"\n",
    "    if until == '':\n",
    "        until = datetime.datetime.strftime(datetime.date.today(), '%Y-%m-%d')\n",
    "    q += f\" until:{until}\"\n",
    "\n",
    "    if since == '':\n",
    "        since = datetime.datetime.strftime(datetime.datetime.strptime(until, '%Y-%m-%d') - datetime.timedelta(days=7), '%Y-%m-%d')\n",
    "\n",
    "    q += f\" since:{since}\"\n",
    "\n",
    "    if retweet == 'y':\n",
    "        q += f\" exclude:retweets\"\n",
    "\n",
    "    if replies == 'y':\n",
    "        q += f\" exclude:replies\"\n",
    "\n",
    "    if username != '' and text != '':\n",
    "        filename = f\"{since}_{until}_{username}_{text}.csv\"\n",
    "\n",
    "    elif username != \"\":\n",
    "        filename = f\"{since}_{until}_{username}.csv\"\n",
    "\n",
    "    else:\n",
    "        filename = f\"{since}_{until}_{text}.csv\"\n",
    "\n",
    "    print(filename)\n",
    "\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-22_2023-03-24_makinde.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12ed3338480b46abb0576b0110b08a81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=makinde+until%3A2023-03-24+since%3A2023-03-22&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: non-200 status code (404)\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=makinde+until%3A2023-03-24+since%3A2023-03-22&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      "Errors: blocked (403), non-200 status code (404), non-200 status code (404), non-200 status code (404)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=makinde+until%3A2023-03-24+since%3A2023-03-22&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mScraperException\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[47], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tqdm_notebook(total\u001B[38;5;241m=\u001B[39mcount) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m---> 15\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i,tweet \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sntwitter\u001B[38;5;241m.\u001B[39mTwitterSearchScraper(q)\u001B[38;5;241m.\u001B[39mget_items()): \u001B[38;5;66;03m#declare a username\u001B[39;00m\n\u001B[0;32m     16\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m i\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39mcount: \u001B[38;5;66;03m#number of tweets you want to scrape\u001B[39;00m\n\u001B[0;32m     17\u001B[0m                 \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:1657\u001B[0m, in \u001B[0;36mTwitterSearchScraper.get_items\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1654\u001B[0m params \u001B[38;5;241m=\u001B[39m paginationParams\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m   1655\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcursor\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m-> 1657\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_api_data(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://api.twitter.com/2/search/adaptive.json\u001B[39m\u001B[38;5;124m'\u001B[39m, _TwitterAPIType\u001B[38;5;241m.\u001B[39mV2, params, paginationParams, cursor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cursor):\n\u001B[0;32m   1658\u001B[0m \t\u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_v2_timeline_instructions_to_tweets_or_users(obj)\n",
      "File \u001B[1;32mC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:761\u001B[0m, in \u001B[0;36m_TwitterAPIScraper._iter_api_data\u001B[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001B[0m\n\u001B[0;32m    759\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    760\u001B[0m \t_logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRetrieving scroll page \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcursor\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 761\u001B[0m \tobj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_api_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapiType\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreqParams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    762\u001B[0m \t\u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m    764\u001B[0m \t\u001B[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:727\u001B[0m, in \u001B[0;36m_TwitterAPIScraper._get_api_data\u001B[1;34m(self, endpoint, apiType, params)\u001B[0m\n\u001B[0;32m    725\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m apiType \u001B[38;5;129;01mis\u001B[39;00m _TwitterAPIType\u001B[38;5;241m.\u001B[39mGRAPHQL:\n\u001B[0;32m    726\u001B[0m \tparams \u001B[38;5;241m=\u001B[39m urllib\u001B[38;5;241m.\u001B[39mparse\u001B[38;5;241m.\u001B[39murlencode({k: json\u001B[38;5;241m.\u001B[39mdumps(v, separators \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m'\u001B[39m)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m params\u001B[38;5;241m.\u001B[39mitems()}, quote_via \u001B[38;5;241m=\u001B[39m urllib\u001B[38;5;241m.\u001B[39mparse\u001B[38;5;241m.\u001B[39mquote)\n\u001B[1;32m--> 727\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apiHeaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponseOkCallback\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_api_response\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    728\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    729\u001B[0m \tobj \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mjson()\n",
      "File \u001B[1;32mC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\Lib\\site-packages\\snscrape\\base.py:251\u001B[0m, in \u001B[0;36mScraper._get\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 251\u001B[0m \t\u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\Lib\\site-packages\\snscrape\\base.py:247\u001B[0m, in \u001B[0;36mScraper._request\u001B[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001B[0m\n\u001B[0;32m    245\u001B[0m \t_logger\u001B[38;5;241m.\u001B[39mfatal(msg)\n\u001B[0;32m    246\u001B[0m \t_logger\u001B[38;5;241m.\u001B[39mfatal(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mErrors: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(errors)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 247\u001B[0m \t\u001B[38;5;28;01mraise\u001B[39;00m ScraperException(msg)\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReached unreachable code\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mScraperException\u001B[0m: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=makinde+until%3A2023-03-24+since%3A2023-03-22&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up."
     ]
    }
   ],
   "source": [
    "#  3. The script should also print the following information:\n",
    "\n",
    "q = search(text,username,since,until,retweet,replies)\n",
    "\n",
    "# Creating list to append tweet data\n",
    "tweets_list1 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "if count == -1:\n",
    "    for i,tweet in enumerate(tqdm_notebook(sntwitter.TwitterSearchScraper(q).get_items())):\n",
    "        tweets_list1.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username,tweet.lang,\n",
    "                             tweet.hashtags,tweet.replyCount,tweet.retweetCount, tweet.likeCount,tweet.quoteCount,tweet.media])\n",
    "else:\n",
    "    with tqdm_notebook(total=count) as pbar:\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(q).get_items()): #declare a username\n",
    "            if i>=count: #number of tweets you want to scrape\n",
    "                break\n",
    "            tweets_list1.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username,tweet.lang,tweet.hashtags,tweet.coordinates,tweet.replyCount,\n",
    "                                  tweet.retweetCount,tweet.likeCount,tweet.quoteCount,tweet.media])\n",
    "            pbar.update(1)\n",
    "\n",
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df1 = pd.DataFrame(tweets_list1, columns=['DateTime', 'TweetId', 'Text', 'Username','Language',\n",
    "                                                 'Hashtags','Cords','ReplyCount','RetweetCount','LikeCount','QuoteCount','Media'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_df1 = pd.read_csv(\"2019-01-01_2023-01-06_oriname.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_df1.head(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweets_df2 = pd.DataFrame(tweets_df1, columns=['DateTime', 'TweetId', 'Text', 'Username','Language',\n",
    "                                                 'Hashtags','Cords','ReplyCount','RetweetCount','LikeCount','QuoteCount','Media'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df2.sort_values(by='DateTime',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop NaN, None rows from Coordinates column\n",
    "tweets_df1 = tweets_df1.dropna(subset=['Cords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dfc = tweets_df1['Cords'].dropna()\n",
    "#dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a new dataframe called tweets_df2 for preprocessing for visualization later\n",
    "tweets_df2 = tweets_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tweets_df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot of heatmap showing locations where the word INEC is being mention around the word using Plotly Express\n",
    "import plotly.express as px\n",
    "\n",
    "tweets_df1['Lat'] = tweets_df1['Cords'].apply(lambda x: x.latitude if x else None)\n",
    "tweets_df1['Lon'] = tweets_df1['Cords'].apply(lambda x: x.longitude if x else None)\n",
    "\n",
    "tweets_df1 = tweets_df1.dropna(subset=['Lat', 'Lon'], how='all')\n",
    "\n",
    "fig = px.density_mapbox(tweets_df1, lat='Lat', lon='Lon', hover_name='Text',\n",
    "                      width=1000, height=600, mapbox_style='stamen-terrain', title='Tweet Locations')\n",
    "\n",
    "# set the map style to OpenStreetMap\n",
    "#fig.update_layout(mapbox_style='stamen-terrain')\n",
    "# show the plot\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import plotly.express as px\n",
    "\n",
    "# initialize geolocator\n",
    "geolocator = Nominatim(user_agent='my-twitter-app')\n",
    "\n",
    "# define a function to get country from latitude and longitude\n",
    "def get_country(lat, lon):\n",
    "    location = geolocator.reverse((lat, lon), exactly_one=True)\n",
    "    if location is None:\n",
    "        return None\n",
    "    else:\n",
    "        return location.raw['address']['country']\n",
    "\n",
    "# apply the get_country function to each row of the tweet data\n",
    "tweets_df1['Country'] = tweets_df1.apply(lambda row: get_country(row['Lat'], row['Lon']), axis=1)\n",
    "\n",
    "# group the data by country and count the number of tweets\n",
    "tweet_counts = tweets_df1.groupby('Country').size().reset_index(name='count')\n",
    "\n",
    "# create a choropleth map of tweet counts by country\n",
    "fig_map = px.choropleth(tweet_counts, locations='Country', locationmode='country names', color='count',\n",
    "                        color_continuous_scale='YlOrRd', range_color=(0, 200),\n",
    "                        title='Tweet Counts by Country')\n",
    "fig_map.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Time series of tweets\n",
    "tweets_df1['Hour'] = tweets_df1['DateTime'].dt.hour\n",
    "\n",
    "tweets_df1['Year'] = tweets_df1['DateTime'].dt.year\n",
    "\n",
    "tweets_df1['Month'] = tweets_df1['DateTime'].dt.month\n",
    "\n",
    "tweets_df1['MonthName'] = tweets_df1['DateTime'].dt.month_name()\n",
    "\n",
    "tweets_df1['MonthDay'] = tweets_df1['DateTime'].dt.day\n",
    "\n",
    "tweets_df1['DayName'] = tweets_df1['DateTime'].dt.day_name()\n",
    "\n",
    "tweets_df1['Week'] = tweets_df1['DateTime'].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The Datetime column contains both date and time, therefore it is better to split data and time in separate columns.\n",
    "tweets_df1['Date'] = [d.date() for d in tweets_df1['DateTime']]\n",
    "\n",
    "tweets_df1['Time'] = [d.time() for d in tweets_df1['DateTime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After splitting we will drop the DateTime column.\n",
    "tweets_df1.drop('DateTime',axis=1,inplace=True)\n",
    "\n",
    "tweets_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finally our data is prepared, we will now save the dataframe as csv using df.to_csv() function which takes filename as an input parameter.\n",
    "tweets_df1.to_csv(f\"{filename}\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"2023-01-01_2023-03-03_INEC.csv\")\n",
    "\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Count by Year - The countplot function of seaborn allows us to plot count of tweets by year.\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "sns.countplot(x= tweets['Year'])\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), (p.get_x()+0.05, p.get_height()+20), fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "\n",
    "\n",
    "ax=plt.subplot(221)\n",
    "\n",
    "sns.lineplot(tweets.Year.value_counts())\n",
    "\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "plt.xticks(np.arange(2023,1))\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(222)\n",
    "\n",
    "sns.histplot(x=tweets.Year,stat='count',binwidth=1,kde='true',discrete=True)\n",
    "\n",
    "plt.xticks(np.arange(2023,1))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "\n",
    "sns.kdeplot(x=tweets.Year,fill=True)\n",
    "\n",
    "plt.xticks(np.arange(2023,1))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "plt.subplot(224)\n",
    "\n",
    "sns.kdeplot(x=tweets.Year,fill=True,bw_adjust=3)\n",
    "\n",
    "plt.xticks(np.arange(2023,1))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['Date'] = pd.to_datetime(tweets['Date'])\n",
    "tweets['Month'] = tweets['Date'].dt.month\n",
    "\n",
    "# plot tweet counts by year and month\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "ax = plt.subplot(221)\n",
    "sns.lineplot(tweets.Month.value_counts().sort_index(), color='red')\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xticks(range(1, 13))\n",
    "\n",
    "plt.subplot(222)\n",
    "sns.histplot(x=tweets.Month, stat='count', binwidth=1, kde=True, discrete=True, color='green')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.xlabel(\"Month\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(223)\n",
    "sns.kdeplot(x=tweets.Month, fill=True, color='blue')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.xlabel(\"Month\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(224)\n",
    "sns.kdeplot(x=tweets.Month, fill=True, bw_adjust=3, color='purple')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.xlabel(\"Month\")\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "sns.countplot(x= tweets['Month'])\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), (p.get_x()+0.05, p.get_height()+20), fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "\n",
    "\n",
    "ax=plt.subplot(221)\n",
    "\n",
    "sns.lineplot(tweets.Month.value_counts())\n",
    "\n",
    "ax.set_xlabel(\"Month\")\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "plt.xticks(np.arange(1,13,1))\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(222)\n",
    "\n",
    "sns.histplot(x=tweets.Month,stat='count',binwidth=1,kde='true',discrete=True)\n",
    "\n",
    "plt.xticks(np.arange(1,13,1))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "\n",
    "sns.kdeplot(x=tweets.Month,fill=True)\n",
    "\n",
    "plt.xticks(np.arange(1,13,1))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(224)\n",
    "\n",
    "sns.kdeplot(x=tweets.Month,fill=True,bw_adjust=3)\n",
    "\n",
    "plt.xticks(np.arange(1,13,1))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "\n",
    "\n",
    "ax=plt.subplot(221)\n",
    "\n",
    "sns.lineplot(tweets.MonthDay.value_counts())\n",
    "\n",
    "ax.set_xlabel(\"MonthDay\")\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(222)\n",
    "\n",
    "sns.histplot(x=tweets.MonthDay,stat='count',binwidth=1,kde='true',discrete=True)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "\n",
    "sns.kdeplot(x=tweets.MonthDay,fill=True)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(224)\n",
    "\n",
    "sns.kdeplot(x=tweets.MonthDay,fill=True,bw_adjust=3)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "sns.countplot(x= tweets['Week'])\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), (p.get_x()+0.005, p.get_height()+5), fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "\n",
    "\n",
    "ax=plt.subplot(221)\n",
    "\n",
    "sns.lineplot(tweets.Week.value_counts())\n",
    "\n",
    "ax.set_xlabel(\"Week\")\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(222)\n",
    "\n",
    "sns.histplot(x=tweets.Week,stat='count',binwidth=1,kde='true',discrete=True)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "\n",
    "sns.kdeplot(x=tweets.Week,fill=True)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(224)\n",
    "\n",
    "sns.kdeplot(x=tweets.Week,fill=True,bw_adjust=3)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "sns.countplot(x= tweets['MonthDay'])\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), (p.get_x()+0.05, p.get_height()+5), fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "\n",
    "\n",
    "ax=plt.subplot(221)\n",
    "\n",
    "sns.lineplot(tweets.MonthDay.value_counts())\n",
    "\n",
    "ax.set_xlabel(\"MonthDay\")\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(222)\n",
    "\n",
    "sns.histplot(x=tweets.MonthDay,stat='count',binwidth=1,kde='true',discrete=True)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "\n",
    "sns.kdeplot(x=tweets.MonthDay,fill=True)\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "sns.countplot(x= tweets['Hour'])\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), (p.get_x()+0.05, p.get_height()+20), fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "\n",
    "\n",
    "ax=plt.subplot(221)\n",
    "\n",
    "sns.lineplot(tweets.Hour.value_counts())\n",
    "\n",
    "ax.set_xlabel(\"Hour\")\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "plt.xticks(np.arange(0,24,1))\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(222)\n",
    "\n",
    "sns.histplot(x=tweets.Hour,stat='count',binwidth=1,kde='true',discrete=True)\n",
    "\n",
    "plt.xticks(np.arange(0,24,1))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "\n",
    "sns.kdeplot(x=tweets.Hour,fill=True)\n",
    "\n",
    "plt.xticks(np.arange(0,24,1))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(224)\n",
    "\n",
    "sns.kdeplot(x=tweets.Hour,fill=True,bw_adjust=3)\n",
    "\n",
    "#plt.xticks(np.arange(0,24,1))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df2[['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df2.to_csv(f\"tweets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pip install wordcloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
