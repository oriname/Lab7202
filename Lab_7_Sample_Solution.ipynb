{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cqfHR5rJt4h",
    "outputId": "0ba0f439-c58e-47d8-bb78-d202afb39459",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: Cython==0.29.32 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from gensim) (0.29.32)\n",
      "Requirement already satisfied: pandas in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7.1)\n",
      "Requirement already satisfied: simpful in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.10.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "AxkfffYtJ7nG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "RhSsvi3lNnYu"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Dictionary with 'data' as key and each review as an element of list\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ZOHKr6vcNwX2"
   },
   "outputs": [],
   "source": [
    "# list of articles\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "uI4Qfp5ENw5-"
   },
   "outputs": [],
   "source": [
    "# Dataframe with a column `document` containing all the articles\n",
    "news_df = pd.DataFrame({'document':documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGjLc78oN0Gb",
    "outputId": "2fae1c1c-9614-4237-bfa7-742916581485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document\n",
      "0  Well i'm not sure about the story nad it did s...\n",
      "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n",
      "2  Although I realize that principle is not one o...\n",
      "3  Notwithstanding all the legitimate fuss about ...\n",
      "4  Well, I will have to change the scoring on my ...\n"
     ]
    }
   ],
   "source": [
    "print(news_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7w6RVreHQTQs",
    "outputId": "4c2c62f7-1ec2-4436-ac5d-dcd0d66accb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oriname\\AppData\\Local\\Temp\\ipykernel_23476\\4185485833.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tokenized_doc = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = news_df[:100]\n",
    "tokenized_doc = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jZUPbA3QV0s",
    "outputId": "c71bb808-6a1b-49f2-b5ed-5fcf03416016"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     well sure about story seem biased what disagre...\n",
       "1     yeah expect people read actually accept hard a...\n",
       "2     although realize that principle your strongest...\n",
       "3     notwithstanding legitimate fuss about this pro...\n",
       "4     well will have change scoring playoff pool unf...\n",
       "                            ...                        \n",
       "95    follow mark last posting current investigation...\n",
       "96    could folk song clementine memory serves part ...\n",
       "97    first chip doesn that runs megabits second whi...\n",
       "98    created image gaea therefore must pinnacle cre...\n",
       "99    sale pontiac grand prix white white rims gray ...\n",
       "Name: document, Length: 100, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_doc(text):\n",
    "    #for token in text:\n",
    "    text =  ' '.join([w.lower() for w in text.split() if len(w)>3])\n",
    "    return text;\n",
    "\n",
    "tokenized_doc = tokenized_doc.apply(clean_doc)\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W1NAWAREQd5T",
    "outputId": "3b9931f1-bcdc-42b0-f3b2-6aca63110258"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Oriname\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Oriname\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     well sure story seem biased disagree statement...\n",
       "1     yeah expect people read actually accept hard a...\n",
       "2     although realize principle strongest points wo...\n",
       "3     notwithstanding legitimate fuss proposal much ...\n",
       "4     well change scoring playoff pool unfortunately...\n",
       "                            ...                        \n",
       "95    follow mark last posting current investigation...\n",
       "96    could folk song clementine memory serves part ...\n",
       "97    first chip runs megabits second beyond need vo...\n",
       "98    created image gaea therefore must pinnacle cre...\n",
       "99    sale pontiac grand prix white white rims gray ...\n",
       "Name: document, Length: 100, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "custom = list(stopwords_set)+list(punctuation)\n",
    "\n",
    "def stopWordRemoval(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = ' '.join([word for word in text if word not in custom])\n",
    "    return text\n",
    "\n",
    "tokenized_doc = tokenized_doc.apply(stopWordRemoval)\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vaNzmfTfQhTx",
    "outputId": "16563136-7dc6-411b-f080-dff2e569ef58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Oriname\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Oriname\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     well sure story seem biased disagree statement...\n",
       "1     yeah expect people read actually accept hard a...\n",
       "2     although realize principle strongest point wou...\n",
       "3     notwithstanding legitimate fuss proposal much ...\n",
       "4     well change scoring playoff pool unfortunately...\n",
       "                            ...                        \n",
       "95    follow mark last posting current investigation...\n",
       "96    could folk song clementine memory serf part go...\n",
       "97    first chip run megabit second beyond need voic...\n",
       "98    created image gaea therefore must pinnacle cre...\n",
       "99    sale pontiac grand prix white white rim gray i...\n",
       "Name: document, Length: 100, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing wordnet lemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('omw-1.4')\n",
    "def lemData(text):\n",
    "    text = word_tokenize(text)\n",
    "    newText = []\n",
    "    for word in text:\n",
    "        newText.append(lemmatizer.lemmatize(word))\n",
    "    return ' '.join(newText)\n",
    "\n",
    "tokenized_doc = tokenized_doc.apply(lemData)\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "4ts_r2OvSMxL",
    "outputId": "66287709-7359-4158-dd84-4f4cf686c56d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126337</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.17339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122917</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2         3    4    5    6         7    8    9    ...  990  991  \\\n",
       "0   0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "1   0.0  0.0  0.0  0.196228  0.0  0.0  0.0  0.167434  0.0  0.0  ...  0.0  0.0   \n",
       "2   0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "3   0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "4   0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "..  ...  ...  ...       ...  ...  ...  ...       ...  ...  ...  ...  ...  ...   \n",
       "95  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "96  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "97  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "98  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "99  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "    992  993  994       995      996  997  998  999  \n",
       "0   0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.126337  0.00000  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "..  ...  ...  ...       ...      ...  ...  ...  ...  \n",
       "95  0.0  0.0  0.0  0.000000  0.17339  0.0  0.0  0.0  \n",
       "96  0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "97  0.0  0.0  0.0  0.122917  0.00000  0.0  0.0  0.0  \n",
       "98  0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "99  0.0  0.0  0.0  0.000000  0.00000  0.0  0.0  0.0  \n",
       "\n",
       "[100 rows x 1000 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialising the tfidf vectorizer with the default stopword list \n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features= 1000, max_df = 0.5, smooth_idf=True)\n",
    "\n",
    "#Vectorizing 'X' column\n",
    "vector =tfidf.fit_transform(tokenized_doc)\n",
    "\n",
    "#Converting vector into an array\n",
    "X= vector.toarray()\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3D4_FDTZSr_Q",
    "outputId": "8e90a74b-be62-48d3-9c98-9b00f7b7e145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of topics chosen are 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=5, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)\n",
    "print(\"The number of topics chosen are\",len(svd_model.components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrRacUn6S5Cz",
    "outputId": "6c7a279b-9f2a-4029-a238-927d21250200"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNqG4BLETAC3",
    "outputId": "25ea0b8c-744d-4ff5-f6a5-242de9ace082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Topic 0: ', 'like', 'know', 'chip', 'think', 'time', 'good', 'year']\n",
      "['Topic 1: ', 'memory', 'cache', 'vram', 'simm', 'chip', 'simms', 'song']\n",
      "['Topic 2: ', 'chip', 'phone', 'government', 'clipper', 'voice', 'key', 'work']\n",
      "['Topic 3: ', 'israel', 'israeli', 'arab', 'memory', 'table', 'medium', 'know']\n",
      "['Topic 4: ', 'vram', 'need', 'jesus', 'really', 'government', 'appears', 'simms']\n"
     ]
    }
   ],
   "source": [
    "terms = tfidf.get_feature_names_out()\n",
    "topics = []\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    topics.append(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        topics.append(t[0])\n",
    "\n",
    "final_topic_list = [topics[i:i+8] for i in range(0, len(topics), 8)]\n",
    "\n",
    "for x in final_topic_list:\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2clqwMevWHDT"
   },
   "source": [
    "Gensim LSI & **LDA** **bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "bLxsZ-ToUwgO"
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BC3KQ3erW2Q4",
    "outputId": "725cbe76-68ab-4276-ad91-2c1df58e3efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['well', 'sure', 'story', 'seem', 'biased', 'disagree', 'statement', 'medium', 'ruin', 'israel', 'reputation', 'rediculous', 'medium', 'israeli', 'medium', 'world', 'lived', 'europe', 'realize', 'incidence', 'described', 'letter', 'occured', 'medium', 'whole', 'seem', 'ignore', 'subsidizing', 'israel', 'existance', 'european', 'least', 'degree', 'think', 'might', 'reason', 'report', 'clearly', 'atrocity', 'shame', 'austria', 'daily', 'report', 'inhuman', 'act', 'commited', 'israeli', 'soldier', 'blessing', 'received', 'government', 'make', 'holocaust', 'guilt', 'away', 'look', 'jew', 'treating', 'race', 'power', 'unfortunate']]\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords_set\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize and remove the stopwords\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "# Creating a list of documents \n",
    "list_of_docs = tokenized_doc.tolist()\n",
    "\n",
    "# Implementing the function for list_of_docs\n",
    "doc_clean = [clean(doc).split() for doc in list_of_docs]\n",
    "print(doc_clean[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_2NYr9nYBIf",
    "outputId": "960cfc4a-2cb8-440d-e136-de5b32649ddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.435*\"graphic\" + 0.249*\"mail\" + 0.228*\"image\" + 0.224*\"file\" + '\n",
      "  '0.191*\"send\" + 0.174*\"object\" + 0.158*\"format\" + 0.157*\"server\" + '\n",
      "  '0.137*\"also\" + 0.124*\"tracer\"'),\n",
      " (1,\n",
      "  '-0.463*\"israel\" + -0.322*\"israeli\" + -0.205*\"soldier\" + -0.204*\"lebanese\" + '\n",
      "  '-0.183*\"peace\" + -0.181*\"village\" + -0.136*\"zone\" + -0.133*\"would\" + '\n",
      "  '-0.122*\"people\" + -0.118*\"time\"'),\n",
      " (2,\n",
      "  '0.231*\"know\" + 0.218*\"would\" + 0.216*\"like\" + 0.210*\"good\" + '\n",
      "  '-0.198*\"israel\" + 0.194*\"patch\" + 0.170*\"contact\" + 0.170*\"bike\" + '\n",
      "  '0.145*\"life\" + 0.143*\"jesus\"'),\n",
      " (3,\n",
      "  '0.348*\"adirondack\" + 0.280*\"john\" + 0.278*\"providence\" + 0.278*\"cape\" + '\n",
      "  '0.278*\"rochester\" + 0.278*\"fredericton\" + 0.278*\"breton\" + '\n",
      "  '0.278*\"baltimore\" + 0.278*\"moncton\" + 0.278*\"utica\"'),\n",
      " (4,\n",
      "  '-0.293*\"patch\" + -0.267*\"contact\" + -0.248*\"bike\" + -0.216*\"force\" + '\n",
      "  '0.193*\"life\" + 0.188*\"good\" + 0.168*\"jesus\" + -0.163*\"motorcycle\" + '\n",
      "  '-0.160*\"wheel\" + -0.160*\"effect\"'),\n",
      " (5,\n",
      "  '-0.249*\"year\" + -0.239*\"moon\" + -0.185*\"lunar\" + -0.171*\"another\" + '\n",
      "  '-0.163*\"tonne\" + -0.145*\"could\" + -0.137*\"much\" + -0.131*\"payload\" + '\n",
      "  '-0.131*\"contest\" + 0.130*\"patch\"'),\n",
      " (6,\n",
      "  '0.279*\"phone\" + 0.194*\"criminal\" + 0.191*\"cipher\" + 0.157*\"mean\" + '\n",
      "  '0.154*\"government\" + 0.153*\"stolen\" + 0.139*\"public\" + 0.136*\"control\" + '\n",
      "  '0.133*\"key\" + 0.125*\"right\"'),\n",
      " (7,\n",
      "  '0.326*\"cable\" + 0.281*\"drive\" + 0.246*\"connector\" + 0.177*\"work\" + '\n",
      "  '0.170*\"power\" + 0.162*\"hard\" + 0.161*\"disk\" + 0.160*\"spare\" + 0.147*\"back\" '\n",
      "  '+ 0.144*\"another\"'),\n",
      " (8,\n",
      "  '-0.281*\"andrew\" + -0.280*\"conference\" + -0.279*\"paper\" + '\n",
      "  '-0.279*\"application\" + -0.189*\"june\" + -0.167*\"mormon\" + '\n",
      "  '-0.140*\"construction\" + -0.140*\"tutorial\" + -0.140*\"programmer\" + '\n",
      "  '-0.140*\"theme\"'),\n",
      " (9,\n",
      "  '-0.319*\"mormon\" + 0.200*\"chip\" + 0.199*\"session\" + 0.197*\"key\" + '\n",
      "  '-0.193*\"belief\" + 0.192*\"door\" + 0.179*\"back\" + -0.149*\"word\" + '\n",
      "  '0.136*\"secret\" + -0.133*\"believe\"')]\n"
     ]
    }
   ],
   "source": [
    "# Creating the dictionary id2word from our cleaned word list doc_clean\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Creating the corpus\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the LSi model\n",
    "lsimodel = LsiModel(corpus=doc_term_matrix, num_topics=10, id2word=dictionary)\n",
    "pprint(lsimodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "al0Y1xmKYKbc",
    "outputId": "b33e14f4-0b53-4518-aac8-808a0915f64a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.024*\"graphic\" + 0.014*\"mail\" + 0.013*\"image\" + 0.012*\"file\" + '\n",
      "  '0.011*\"send\" + 0.010*\"object\" + 0.009*\"server\" + 0.009*\"format\" + '\n",
      "  '0.007*\"also\" + 0.007*\"tracer\"'),\n",
      " (1,\n",
      "  '0.010*\"mormon\" + 0.010*\"application\" + 0.009*\"would\" + 0.008*\"conference\" + '\n",
      "  '0.007*\"paper\" + 0.007*\"question\" + 0.006*\"year\" + 0.006*\"andrew\" + '\n",
      "  '0.006*\"belief\" + 0.006*\"israel\"'),\n",
      " (2,\n",
      "  '0.017*\"bike\" + 0.007*\"think\" + 0.007*\"good\" + 0.007*\"radio\" + '\n",
      "  '0.007*\"course\" + 0.007*\"give\" + 0.007*\"motorcycle\" + 0.007*\"insurance\" + '\n",
      "  '0.004*\"also\" + 0.004*\"much\"'),\n",
      " (3,\n",
      "  '0.015*\"torrey\" + 0.015*\"power\" + 0.012*\"panther\" + 0.010*\"repeat\" + '\n",
      "  '0.008*\"florida\" + 0.008*\"stress\" + 0.006*\"name\" + 0.006*\"first\" + '\n",
      "  '0.006*\"president\" + 0.006*\"turmeric\"'),\n",
      " (4,\n",
      "  '0.012*\"neon\" + 0.011*\"like\" + 0.009*\"starter\" + 0.009*\"scope\" + '\n",
      "  '0.007*\"find\" + 0.007*\"well\" + 0.007*\"strip\" + 0.007*\"lamp\" + 0.007*\"little\" '\n",
      "  '+ 0.007*\"u\"'),\n",
      " (5,\n",
      "  '0.015*\"father\" + 0.012*\"always\" + 0.012*\"love\" + 0.012*\"heaven\" + '\n",
      "  '0.012*\"heart\" + 0.009*\"test\" + 0.009*\"bible\" + 0.009*\"child\" + 0.006*\"like\" '\n",
      "  '+ 0.006*\"would\"'),\n",
      " (6,\n",
      "  '0.011*\"good\" + 0.011*\"know\" + 0.009*\"life\" + 0.008*\"jesus\" + 0.008*\"cable\" '\n",
      "  '+ 0.008*\"drive\" + 0.007*\"people\" + 0.007*\"time\" + 0.007*\"better\" + '\n",
      "  '0.007*\"master\"'),\n",
      " (7,\n",
      "  '0.011*\"creation\" + 0.007*\"used\" + 0.007*\"condition\" + 0.007*\"cost\" + '\n",
      "  '0.007*\"card\" + 0.007*\"voice\" + 0.007*\"circuit\" + 0.007*\"chip\" + '\n",
      "  '0.007*\"accept\" + 0.005*\"time\"'),\n",
      " (8,\n",
      "  '0.027*\"adirondack\" + 0.022*\"john\" + 0.022*\"springfield\" + 0.022*\"utica\" + '\n",
      "  '0.022*\"moncton\" + 0.022*\"rochester\" + 0.022*\"providence\" + 0.022*\"cape\" + '\n",
      "  '0.022*\"breton\" + 0.022*\"baltimore\"'),\n",
      " (9,\n",
      "  '0.013*\"coast\" + 0.009*\"though\" + 0.009*\"really\" + 0.009*\"even\" + '\n",
      "  '0.009*\"oregon\" + 0.009*\"portland\" + 0.009*\"understanding\" + 0.005*\"exist\" + '\n",
      "  '0.005*\"appears\" + 0.004*\"nice\"'),\n",
      " (10,\n",
      "  '0.010*\"well\" + 0.009*\"would\" + 0.009*\"think\" + 0.009*\"something\" + '\n",
      "  '0.009*\"keyboard\" + 0.007*\"like\" + 0.006*\"time\" + 0.006*\"weapon\" + '\n",
      "  '0.006*\"block\" + 0.005*\"much\"'),\n",
      " (11,\n",
      "  '0.027*\"israel\" + 0.019*\"israeli\" + 0.011*\"cache\" + 0.011*\"peace\" + '\n",
      "  '0.011*\"soldier\" + 0.011*\"lebanese\" + 0.010*\"village\" + 0.010*\"card\" + '\n",
      "  '0.007*\"zone\" + 0.006*\"occupied\"'),\n",
      " (12,\n",
      "  '0.009*\"good\" + 0.007*\"vram\" + 0.006*\"look\" + 0.006*\"post\" + '\n",
      "  '0.006*\"technology\" + 0.006*\"back\" + 0.005*\"well\" + 0.005*\"reading\" + '\n",
      "  '0.005*\"come\" + 0.005*\"return\"'),\n",
      " (13,\n",
      "  '0.010*\"since\" + 0.010*\"mary\" + 0.010*\"bird\" + 0.008*\"going\" + 0.006*\"time\" '\n",
      "  '+ 0.006*\"original\" + 0.006*\"tradition\" + 0.006*\"guess\" + 0.006*\"conception\" '\n",
      "  '+ 0.006*\"thing\"'),\n",
      " (14,\n",
      "  '0.023*\"patch\" + 0.021*\"contact\" + 0.019*\"bike\" + 0.016*\"force\" + '\n",
      "  '0.013*\"would\" + 0.012*\"effect\" + 0.012*\"wheel\" + 0.012*\"motorcycle\" + '\n",
      "  '0.011*\"like\" + 0.008*\"vehicle\"'),\n",
      " (15,\n",
      "  '0.007*\"would\" + 0.007*\"mattress\" + 0.007*\"suresh\" + 0.007*\"pick\" + '\n",
      "  '0.004*\"fit\" + 0.004*\"come\" + 0.004*\"good\" + 0.004*\"mail\" + 0.004*\"image\" + '\n",
      "  '0.004*\"stuff\"'),\n",
      " (16,\n",
      "  '0.009*\"cache\" + 0.006*\"lunar\" + 0.006*\"right\" + 0.005*\"chip\" + '\n",
      "  '0.005*\"memory\" + 0.005*\"time\" + 0.005*\"government\" + 0.005*\"concentration\" '\n",
      "  '+ 0.005*\"earth\" + 0.005*\"small\"'),\n",
      " (17,\n",
      "  '0.019*\"port\" + 0.008*\"plus\" + 0.008*\"device\" + 0.008*\"printer\" + '\n",
      "  '0.008*\"battery\" + 0.008*\"serial\" + 0.008*\"worked\" + 0.008*\"connected\" + '\n",
      "  '0.008*\"building\" + 0.004*\"thinking\"'),\n",
      " (18,\n",
      "  '0.011*\"phone\" + 0.009*\"last\" + 0.009*\"year\" + 0.008*\"good\" + '\n",
      "  '0.008*\"government\" + 0.006*\"mean\" + 0.006*\"cipher\" + 0.006*\"criminal\" + '\n",
      "  '0.006*\"cjackson\" + 0.006*\"control\"'),\n",
      " (19,\n",
      "  '0.009*\"hole\" + 0.009*\"back\" + 0.009*\"another\" + 0.009*\"like\" + 0.008*\"key\" '\n",
      "  '+ 0.007*\"year\" + 0.007*\"moon\" + 0.007*\"session\" + 0.007*\"could\" + '\n",
      "  '0.006*\"chip\"')]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Creating the dictionary id2word from our cleaned word list doc_clean\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Creating the corpus\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the LDA model\n",
    "ldamodel = LdaModel(corpus=doc_term_matrix, num_topics=20,id2word=dictionary, random_state=20, passes=30)\n",
    "\n",
    "# printing the topics\n",
    "pprint(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cxt6XXMYOYX",
    "outputId": "4cda4148-238f-4b84-815c-b9af7b4b4e46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.026778300427114\n",
      "\n",
      "Coherence Score:  0.40762641452983245\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "perplexity_lda = ldamodel.log_perplexity(doc_term_matrix)\n",
    "print('\\nPerplexity: ', perplexity_lda)  \n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "8CQ703aXZR5h",
    "outputId": "e0fb5fcd-0eac-4a45-c89f-e22481af7976"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stopwords,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = ldamodel.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "fig.savefig('word_cloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
