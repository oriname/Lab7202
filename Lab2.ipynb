{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**bold text**# **Lab Session 2: Introduction to NLP**"
   ],
   "metadata": {
    "id": "dpcpapP5kpLo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by importing NLTK"
   ],
   "metadata": {
    "id": "QhdLFgtclDhq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.corpus import webtext\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "metadata": {
    "id": "sxwaOmVelMAe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mount your Google Drive files"
   ],
   "metadata": {
    "id": "VWeMGT5wlOyW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSYh7b05ldGW",
    "outputId": "ffc0ada2-6a80-4198-af80-d7e9defa370a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the corpus file from Google Drive"
   ],
   "metadata": {
    "id": "bYQJdLWolfzP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "corpus = open('gdrive/My Drive/Colab Notebooks/ObamaSpeech.txt', 'r').read()\n",
    "print(corpus)"
   ],
   "metadata": {
    "id": "rNluBAcolzFF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Tokenize sentences and words, remove stopwords, use stemmer & lemmatizer**\n",
    "First, a note on the difference between Stemming vs Lemmatization:\n",
    "\n",
    "Stemming: Trying to shorten a word with simple regex rules\n",
    "\n",
    "Lemmatization: Trying to find the root word with linguistics rules (with the use of regex rules)"
   ],
   "metadata": {
    "id": "8dE7igp2l-6B"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.corpus.reader import wordlist\n",
    "sentences=nltk.sent_tokenize(corpus)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    \n",
    "\n",
    "words=nltk.word_tokenize(sentences[3])\n",
    "\n",
    "for word in words:\n",
    "    print(word)\n",
    "filtered_sentence=[]\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "print(filtered_sentence)\n",
    "\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "word=filtered_sentence[12]\n",
    "print(word)\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "steemer=PorterStemmer()\n",
    "\n",
    "print(\"The lemma of the word is\", lemmatizer.lemmatize(word,pos=wordnet.VERB))\n",
    "print(\"The stem of the word is\", steemer.stem(word))\n"
   ],
   "metadata": {
    "id": "HpR-gz7LmIYW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk import word_tokenize\n",
    "lemmatized = [[lemmatizer.lemmatize(word) for word in word_tokenize(s)]\n",
    "              for s in filtered_sentence]\n",
    "print(lemmatized)\n"
   ],
   "metadata": {
    "id": "P3WVobCGpVyM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fq=FreqDist(token.lower()for token in filtered_sentence)\n",
    "fq"
   ],
   "metadata": {
    "id": "S-hydJKiqHg9"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
