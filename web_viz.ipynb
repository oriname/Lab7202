{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "  Using cached networkx-3.0-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-3.0\n",
      "Requirement already satisfied: snscrape in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (0.6.0.20230303)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from snscrape) (2.28.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from snscrape) (4.9.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from snscrape) (4.11.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from snscrape) (3.9.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from beautifulsoup4->snscrape) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->snscrape) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->snscrape) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->snscrape) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->snscrape) (2022.12.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: plotly in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.13.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from plotly) (8.2.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: Cython==0.29.32 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from gensim) (0.29.32)\n",
      "Requirement already satisfied: pandas in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7.1)\n",
      "Requirement already satisfied: simpful in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.10.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
      "Requirement already satisfied: numpy in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.24.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.6.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\oriname\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oriname\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: geopy in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\dev\\desktop\\lab7202\\venv\\lib\\site-packages (from geopy) (2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx\n",
    "!pip install snscrape\n",
    "!pip install tqdm\n",
    "!pip install plotly\n",
    "!pip install gensim\n",
    "!pip install numpy\n",
    "!pip install pillow\n",
    "!pip install matplotlib\n",
    "!pip install geopy\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#  1. Write a python script to scrape tweets from twitter using snscrape library. The script should take the following inputs from the user:\n",
    "\n",
    "text = input('Enter query text to be matched (or leave it blank by pressing enter)')\n",
    "\n",
    "username = input('Enter specific username(s) from a twitter account without @ (or leave it blank by pressing enter): ')\n",
    "\n",
    "since = input('Enter startdate in this format yyyy-mm-dd (or leave it blank by pressing enter): ')\n",
    "\n",
    "until = input('Enter enddate in this format yyyy-mm-dd (or leave it blank by pressing enter): ')\n",
    "\n",
    "count = int(input('Enter max number of tweets or enter -1 to retrieve all possible tweets: '))\n",
    "\n",
    "retweet = input('Exclude Retweets? (y/n): ')\n",
    "\n",
    "replies = input('Exclude Replies? (y/n): ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#  2. The script should save the scraped tweets in a csv:\n",
    "\n",
    "def search(text, username, since, until, retweet, replies):\n",
    "    global filename\n",
    "    q = text\n",
    "    if username != '':\n",
    "        q += f\" from:{username}\"\n",
    "    if until == '':\n",
    "        until = datetime.datetime.strftime(datetime.date.today(), '%Y-%m-%d')\n",
    "    q += f\" until:{until}\"\n",
    "\n",
    "    if since == '':\n",
    "        since = datetime.datetime.strftime(datetime.datetime.strptime(until, '%Y-%m-%d') - datetime.timedelta(days=7), '%Y-%m-%d')\n",
    "\n",
    "    q += f\" since:{since}\"\n",
    "\n",
    "    if retweet == 'y':\n",
    "        q += f\" exclude:retweets\"\n",
    "\n",
    "    if replies == 'y':\n",
    "        q += f\" exclude:replies\"\n",
    "\n",
    "    if username != '' and text != '':\n",
    "        filename = f\"{since}_{until}_{username}_{text}.csv\"\n",
    "\n",
    "    elif username != \"\":\n",
    "        filename = f\"{since}_{until}_{username}.csv\"\n",
    "\n",
    "    else:\n",
    "        filename = f\"{since}_{until}_{text}.csv\"\n",
    "\n",
    "    print(filename)\n",
    "\n",
    "    return q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-02_2023-03-24_obi.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7267c66b87f74eb486b3b57ce5262264"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=obi+until%3A2023-03-24+since%3A2023-03-02&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: non-200 status code (404)\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=obi+until%3A2023-03-24+since%3A2023-03-02&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      "Errors: non-200 status code (404), non-200 status code (404), non-200 status code (404), non-200 status code (404)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=obi+until%3A2023-03-24+since%3A2023-03-02&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mScraperException\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tqdm_notebook(total\u001B[38;5;241m=\u001B[39mcount) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m---> 15\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i,tweet \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sntwitter\u001B[38;5;241m.\u001B[39mTwitterSearchScraper(q)\u001B[38;5;241m.\u001B[39mget_items()): \u001B[38;5;66;03m#declare a username\u001B[39;00m\n\u001B[0;32m     16\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m i\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39mcount: \u001B[38;5;66;03m#number of tweets you want to scrape\u001B[39;00m\n\u001B[0;32m     17\u001B[0m                 \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:1657\u001B[0m, in \u001B[0;36mTwitterSearchScraper.get_items\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1654\u001B[0m params \u001B[38;5;241m=\u001B[39m paginationParams\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m   1655\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcursor\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m-> 1657\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_api_data(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://api.twitter.com/2/search/adaptive.json\u001B[39m\u001B[38;5;124m'\u001B[39m, _TwitterAPIType\u001B[38;5;241m.\u001B[39mV2, params, paginationParams, cursor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cursor):\n\u001B[0;32m   1658\u001B[0m \t\u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_v2_timeline_instructions_to_tweets_or_users(obj)\n",
      "File \u001B[1;32mC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:761\u001B[0m, in \u001B[0;36m_TwitterAPIScraper._iter_api_data\u001B[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001B[0m\n\u001B[0;32m    759\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    760\u001B[0m \t_logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRetrieving scroll page \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcursor\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 761\u001B[0m \tobj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_api_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapiType\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreqParams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    762\u001B[0m \t\u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m    764\u001B[0m \t\u001B[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:727\u001B[0m, in \u001B[0;36m_TwitterAPIScraper._get_api_data\u001B[1;34m(self, endpoint, apiType, params)\u001B[0m\n\u001B[0;32m    725\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m apiType \u001B[38;5;129;01mis\u001B[39;00m _TwitterAPIType\u001B[38;5;241m.\u001B[39mGRAPHQL:\n\u001B[0;32m    726\u001B[0m \tparams \u001B[38;5;241m=\u001B[39m urllib\u001B[38;5;241m.\u001B[39mparse\u001B[38;5;241m.\u001B[39murlencode({k: json\u001B[38;5;241m.\u001B[39mdumps(v, separators \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m'\u001B[39m)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m params\u001B[38;5;241m.\u001B[39mitems()}, quote_via \u001B[38;5;241m=\u001B[39m urllib\u001B[38;5;241m.\u001B[39mparse\u001B[38;5;241m.\u001B[39mquote)\n\u001B[1;32m--> 727\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apiHeaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponseOkCallback\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_api_response\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    728\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    729\u001B[0m \tobj \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mjson()\n",
      "File \u001B[1;32mC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\Lib\\site-packages\\snscrape\\base.py:251\u001B[0m, in \u001B[0;36mScraper._get\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 251\u001B[0m \t\u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Users\\Dev\\Desktop\\Lab7202\\venv\\Lib\\site-packages\\snscrape\\base.py:247\u001B[0m, in \u001B[0;36mScraper._request\u001B[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001B[0m\n\u001B[0;32m    245\u001B[0m \t_logger\u001B[38;5;241m.\u001B[39mfatal(msg)\n\u001B[0;32m    246\u001B[0m \t_logger\u001B[38;5;241m.\u001B[39mfatal(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mErrors: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(errors)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 247\u001B[0m \t\u001B[38;5;28;01mraise\u001B[39;00m ScraperException(msg)\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReached unreachable code\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mScraperException\u001B[0m: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=obi+until%3A2023-03-24+since%3A2023-03-02&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up."
     ]
    }
   ],
   "source": [
    "#  3. The script should also print the following information:\n",
    "\n",
    "q = search(text,username,since,until,retweet,replies)\n",
    "\n",
    "# Creating list to append tweet data\n",
    "tweets_list1 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "if count == -1:\n",
    "    for i,tweet in enumerate(tqdm_notebook(sntwitter.TwitterSearchScraper(q).get_items())):\n",
    "        tweets_list1.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username,tweet.lang,\n",
    "                             tweet.hashtags,tweet.replyCount,tweet.retweetCount, tweet.likeCount,tweet.quoteCount,tweet.media])\n",
    "else:\n",
    "    with tqdm_notebook(total=count) as pbar:\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(q).get_items()): #declare a username\n",
    "            if i>=count: #number of tweets you want to scrape\n",
    "                break\n",
    "            tweets_list1.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username,tweet.lang,tweet.hashtags,tweet.coordinates,tweet.replyCount,\n",
    "                                  tweet.retweetCount,tweet.likeCount,tweet.quoteCount,tweet.media])\n",
    "            pbar.update(1)\n",
    "\n",
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df1 = pd.DataFrame(tweets_list1, columns=['DateTime', 'TweetId', 'Text', 'Username','Language',\n",
    "                                                 'Hashtags','Cords','ReplyCount','RetweetCount','LikeCount','QuoteCount','Media'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
